// 1.1 torch.optim.Adam
// Adam 优化器,能够利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。
// 功能一致，torch 参数更多
merge (: framework {name: 'PyTorch', version: '1.5.0'});
merge (: class {framework: 'PyTorch', name: 'torch'});
merge (: class {framework: 'PyTorch', name: 'optim'});
merge (: operator {framework: 'PyTorch', name: 'Adam', full_name: 'torch.optim.Adam'});
merge (: parameter {framework: 'PyTorch', operator: 'torch.optim.Adam', parameter_order: 1, name: 'params', dtype: 'iterable'});
merge (: parameter {framework: 'PyTorch', operator: 'torch.optim.Adam', parameter_order: 2, name: 'lr', dtype: 2, default: "1e-3", optional: 'True'});
merge (: childParameter {framework: 'PyTorch', operator: 'torch.optim.Adam', dtype_order: 1, parameter_order: 2, name: 'lr', dtype: 'float'});
merge (: childParameter {framework: 'PyTorch', operator: 'torch.optim.Adam', dtype_order: 2, parameter_order: 2, name: 'lr', dtype: 'Tensor'});
merge (: parameter {framework: 'PyTorch', operator: 'torch.optim.Adam', parameter_order: 3, name: 'betas', dtype: 'tuple', default: [0.9, 0.999], optional: 'True'});
merge (: parameter {framework: 'PyTorch', operator: 'torch.optim.Adam', parameter_order: 4, name: 'eps', dtype: 'float', default: "1e-8", optional: 'True'});
merge (: parameter {framework: 'PyTorch', operator: 'torch.optim.Adam', parameter_order: 5, name: 'weight_decay', dtype: 'float', default: 0, optional: 'True'});
merge (: parameter {framework: 'PyTorch', operator: 'torch.optim.Adam', parameter_order: 6, name: 'amsgrad', dtype: 'bool', default: 'False', optional: 'true'});



// 1.2 paddle.optimizer.Adam
merge (: framework {name: 'PaddlePaddle', version: '1.5.0'});
merge (: class {framework: 'PaddlePaddle', name: 'paddle'});
merge (: class {framework: 'PaddlePaddle', name: 'optimizer'});
merge (: operator {framework: 'PaddlePaddle', name: 'Adam', full_name: 'paddle.optimizer.Adam'});
merge (: parameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', parameter_order: 1, name: 'learning_rate', dtype: 'float', default: 0.001});
merge (: parameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', parameter_order: 2, name: 'beta1', dtype: 2, default: 0.9});
merge (: childParameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', dtype_order: 1, parameter_order: 2, name: 'beta1', dtype: 'float'});
merge (: childParameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', dtype_order: 2, parameter_order: 2, name: 'beta1', dtype: 'Tensor'});
merge (: parameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', parameter_order: 3, name: 'beta2', dtype: 2, default: 0.999});
merge (: childParameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', dtype_order: 1, parameter_order: 3, name: 'beta2', dtype: 'float'});
merge (: childParameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', dtype_order: 2, parameter_order: 3, name: 'beta2', dtype: 'Tensor'});
merge (: parameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', parameter_order: 4, name: 'epsilon', dtype : 'float', default : '1e-8'});
merge (: parameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', parameter_order: 5, name: 'parameters', dtype: 'list', default: 'None'});
merge (: parameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam', parameter_order: 6, name: 'weight_decay', dtype: 'float', default: 'None'});



// 2. relationship within the framework
// 2.1 torch.optim.Adam
match
	(m1: framework {name: 'PyTorch'}),
	(m2: class {framework: 'PyTorch', name: 'torch'}),
	(m3: class {framework: 'PyTorch', name: 'optim'}),
	(m4: operator {full_name: 'torch.optim.Adam'})
merge (m1) -[: classOfFramework {name: m2.name}]-> (m2)
merge (m2) -[: subClassOfClass {name: m3.name}]-> (m3)
merge (m3) -[: operatorOfClass {name: m4.name}]-> (m4);

match
	(m21: operator {full_name: 'torch.optim.Adam'}),
	(m22: parameter {framework: 'PyTorch', operator: 'torch.optim.Adam'})
merge (m21) -[: parameterOfOperator {parameter_order: m22.parameter_order, name: m22.name}] -> (m22);

match
	(m21: parameter {framework: 'PyTorch', operator: 'torch.optim.Adam'}),
	(m22: childParameter {framework: 'PyTorch', operator: 'torch.optim.Adam'})
where m21.name = m22.name
merge (m21) -[: oneOf {dtype_order: m22.dtype_order, dtype: m22.dtype}] -> (m22);


// 2.2 paddle.optimizer.Adam
match
	(m1: framework {name: 'PaddlePaddle'}),
	(m2: class {framework: 'PaddlePaddle', name: 'paddle'}),
	(m3: class {framework: 'PaddlePaddle', name: 'optimizer'}),
	(m4: operator {full_name: 'paddle.optimizer.Adam'})
merge (m1) -[: classOfFramework {name: m2.name}]-> (m2)
merge (m2) -[: subClassOfClass {name: m3.name}]-> (m3)
merge (m3) -[: operatorOfClass {name: m4.name}]-> (m4);

match
	(m21: operator {full_name: 'paddle.optimizer.Adam'}),
	(m22: parameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam'})
merge (m21) -[: parameterOfOperator {parameter_order: m22.parameter_order, name: m22.name}] -> (m22);

match
	(m21: parameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam'}),
	(m22: childParameter {framework: 'PaddlePaddle', operator: 'paddle.optimizer.Adam'})
where m21.name = m22.name
merge (m21) -[: oneOf {dtype_order: m22.dtype_order, dtype: m22.dtype}] -> (m22);



// 3. relationship between frameworks
match
	(m31: operator {full_name: 'torch.optim.Adam'}),
	(m32: operator {full_name: 'paddle.optimizer.Adam'})
merge (m31) -[: equivalentOperator {framework_name: 'PaddlePaddle'}]-> (m32)
merge (m32) -[: equivalentOperator {framework_name: 'PyTorch'}]-> (m31);

match
	(m31: parameter {framework: 'PyTorch', operator: "torch.optim.Adam", parameter_order: 1}),
	(m32: parameter {framework: 'PaddlePaddle', operator: "paddle.optimizer.Adam", parameter_order: 1})
merge (m31) -[: equivalentParameter {framework_name: 'PaddlePaddle'}]-> (m32)
merge (m32) -[: equivalentParameter {framework_name: 'PyTorch'}]-> (m31);

match
	(m31: parameter {framework: 'PyTorch', operator: "torch.optim.Adam", parameter_order: 2}),
	(m32: parameter {framework: 'PaddlePaddle', operator: "paddle.optimizer.Adam", parameter_order: 2})
merge (m31) -[: equivalentParameter {framework_name: 'PaddlePaddle'}]-> (m32)
merge (m32) -[: equivalentParameter {framework_name: 'PyTorch'}]-> (m31);

match
	(m31: parameter {framework: 'PyTorch', operator: "torch.optim.Adam", parameter_order: 4}),
	(m32: parameter {framework: 'PaddlePaddle', operator: "paddle.optimizer.Adam", parameter_order: 5})
merge (m31) -[: equivalentParameter {framework_name: 'PaddlePaddle'}]-> (m32)
merge (m32) -[: equivalentParameter {framework_name: 'PyTorch'}]-> (m31);

match
	(m31: parameter {framework: 'PyTorch', operator: "torch.optim.Adam", parameter_order: 5}),
	(m32: parameter {framework: 'PaddlePaddle', operator: "paddle.optimizer.Adam", parameter_order: 6})
merge (m31) -[: equivalentParameter {framework_name: 'PaddlePaddle'}]-> (m32)
merge (m32) -[: equivalentParameter {framework_name: 'PyTorch'}]-> (m31);